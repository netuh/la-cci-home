@@include('header.htm')

@@include('blocks/navigation.htm')

@@include('blocks/page-title.htm',{
	"title"		: "Tutorial: Isabela Albuquerque",
	"page"		: "Tutorials"
})

<section class="section single-speaker">
	<div class="container">
		<div class="block">
			<div class="row">
				<div class="col-lg-5 col-md-6 align-self-md-center">
					<div class="image-block">
						<img src="images/keynotes/Isabela.jpg" class="img-fluid" alt="speaker">
					</div>
				</div>
				<div class="col-lg-7 col-md-6 align-self-center">
					<div class="content-block">
						<div class="name">
							<h3>On generalization out-of-distribution and the multiple faces of neural networks robustness</h3>
						</div>
						<div class="profession">
							<p>Dr. Isabela Albuquerque</p>
						</div>
						<div class="details">
							<p>
								Supervised learning under the risk minimization framework is based on the assumption that training and testing data are drawn from the same distribution. However, real-world machine learning applications are often susceptible to distribution shifts, which can occur due to changes in the data collection conditions, adversarial attacks, or other factors. In this tutorial, we will discuss distribution shifts that affect neural networks performance in-the-wild and provide an overview of the different settings often considered in the literature. We will then focus on the ubiquitous domain generalization case, which applies to scenarios where the data generating process at test time may yield samples from never-before-seen distributions. We will cover both theoretical results and techniques to improve neural networks performance for this setting.
							</p>
						</div>
						
					</div>
				</div>
			</div>
		</div>
	</div>
</section>

@@include('blocks/footer.htm')

@@include('footer.htm')